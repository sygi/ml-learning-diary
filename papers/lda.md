done: 29.06.2015

Paper describes Latent Dirichlet Allocation - the probabilistic graphical model used mainly as a generative model of a corpora of documents. It improves over approaches like tf-idf or (p)LSI in a way that it states that each document is generated by a mixture of topics, without the assumption that each document has proportion of topic equal.

In the model, we assume number of words of a document to be a poisson variable, the vector \phi of its topics to be a k (fixed)-dimensional Dirichlet vector, topic is taken as a \phi multinomial variable, and later a word is taken from the topic based on matrix beta explaining belonging of words to the topics.

Later, paper compares LDA to unigram model (where all the words are randomly generated, based on their frequency), mixture of unigrams, where there is a latent topic variable for each document and pLSI, where training document label influences choice of a mixture of topics.

As finding the posterior in LDA is intractable, there are a few approaches to approximate posterior - paper describes variational inference one - using Jensen's inequality ' to adjust the log-likelihood. 

Parameter estimation in LDS is done using variational EM algorithm.

As the LDA model is defined to give 0 likelihood on unseen words, it is adjusted to handle this situation by "smoothing". 

Later, paper shows an example of LDA data and empirical results, comparing it to the previously mentioned models.
